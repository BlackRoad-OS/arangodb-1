{"id":"armadillo-10","content_hash":"e0449bb5ec083d5db93653b7659d2fdd4491e9e146ea377f1b4c4a1c68f057e8","title":"Code cleanup and polish","description":"Final cleanup pass to improve code quality: introduce value objects, centralize magic constants, fix naming consistency, and verify performance.","design":"1. Introduce value objects (reduce primitive obsession):\n   - ServerId (string with validation)\n   - Port (integer with range validation)\n   - DeploymentId (string with validation)\n   - Replace primitive types with value objects\n   - Update type hints throughout\n   \n2. Centralize magic constants:\n   - Audit for hardcoded values (timeouts, ports, paths)\n   - Move to InfrastructureConfig or appropriate config\n   - Document rationale for each value\n   \n3. Fix naming consistency:\n   - Audit _private_method usage\n   - Ensure private methods aren't called externally\n   - Standardize public API naming conventions\n   - Remove any inconsistent patterns\n   \n4. Performance testing:\n   - Benchmark refactored code vs baseline\n   - Ensure no regressions\n   - Profile hot paths\n   - Document performance characteristics","acceptance_criteria":"- Value objects introduced for key types\n- All magic constants centralized in config\n- Naming conventions consistent\n- Performance benchmarks show no regressions\n- Documentation updated\n- All tests passing","notes":"Progress - Magic Constants Eliminated:\n✅ DeploymentOrchestrator: Now uses TimeoutConfig for all operations\n✅ InstanceManager: All public methods use config-based timeouts\n✅ Dynamic timeout calculation based on actual server count (not magic 4x multiplier)\n✅ Eliminated 15+ hardcoded timeout values in critical deployment paths\n\nResults:\n- All timeouts centralized in TimeoutConfig\n- Timeouts scale dynamically with deployment size\n- Better logging shows calculated timeouts\n- All 494 tests passing\n\nRemaining (from original plan):\n- Value objects (ServerId, Port, DeploymentId)\n- Naming consistency audit  \n- Performance benchmarking\n\nCurrent status: Targeted approach successful for timeouts (most impactful).\nDecision needed: Continue with value objects/naming or consider sufficient?","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-10-25T11:25:08.686707002+02:00","updated_at":"2025-10-26T11:28:50.62475364+01:00","closed_at":"2025-10-26T11:28:50.62475364+01:00","source_repo":"."}
{"id":"armadillo-11","content_hash":"f6171e4838ca0a01e31415ddf80f22352b9051c56c1549016d5e64a4734f4e92","title":"Introduce ServerId value object to replace string primitives","description":"Replace string primitive for server_id with a proper value object to add type safety and validation.\n\nCurrently server_id is typed as `str` in ~27+ locations across instances/ module, leading to potential bugs where any string can be passed.\n\nA ServerId value object would:\n- Prevent empty/invalid server IDs\n- Make the domain model more explicit\n- Catch errors at construction time rather than runtime\n- Self-document the code","design":"1. Create armadillo/core/value_objects.py with ServerId class\n2. Update type hints in instances/server.py, manager.py, server_registry.py, deployment_planner.py (~27 locations total)\n3. Update factory methods to construct ServerId objects\n4. Update comparison logic: `server_id == \"foo\"` → `server_id.value == \"foo\"`\n5. Update all tests to use ServerId constructor\n\nImplementation notes:\n- ServerId validates non-empty alphanumeric identifiers\n- Implements __eq__, __hash__ for use in dicts/sets\n- Provides .value property for string access","acceptance_criteria":"- ServerId class implemented with validation\n- All type hints updated from `str` to `ServerId`\n- All construction sites create ServerId objects\n- All comparison logic updated\n- All tests passing\n- No string primitives used for server identifiers","status":"closed","priority":4,"issue_type":"chore","created_at":"2025-10-26T11:29:02.64005518+01:00","updated_at":"2025-11-03T13:49:32.075930099+01:00","closed_at":"2025-11-03T13:49:32.075930099+01:00","source_repo":"."}
{"id":"armadillo-12","content_hash":"9fc9a484dc98cd808b0a0537ea086a97129e90eac9fea7b52a852c1acd0a3f54","title":"Introduce Port value object to replace int primitives","description":"Replace integer primitive for ports with a proper value object to add validation and prevent invalid port numbers.\n\nCurrently port is typed as `int` in ~13+ locations, allowing any integer including invalid port numbers (\u003c 1024 or \u003e 65535).\n\nA Port value object would:\n- Enforce valid port range (1024-65535)\n- Prevent privileged ports (\u003c 1024) from being used accidentally\n- Make port allocation logic more explicit\n- Catch invalid ports at construction time","design":"1. Create Port class in armadillo/core/value_objects.py\n2. Update type hints in instances/server.py, utils/ports.py, deployment_plan.py, core/types.py (~13 locations)\n3. Update PortManager/PortAllocator to return Port objects\n4. Update all port arithmetic: `port + 1` → `Port(port.value + 1)`\n5. Update command builders and server configs to use port.value\n6. Update all tests\n\nImplementation notes:\n- Port validates range 1024-65535\n- Implements __int__, __eq__, __hash__\n- Provides .value property for int access","acceptance_criteria":"- Port class implemented with range validation\n- All type hints updated from `int` to `Port`\n- PortManager returns Port objects\n- All port construction validated\n- All tests passing\n- No raw integers used for ports","status":"open","priority":4,"issue_type":"chore","created_at":"2025-10-26T11:29:06.094969191+01:00","updated_at":"2025-10-26T11:29:06.094969191+01:00","source_repo":"."}
{"id":"armadillo-13","content_hash":"e5c0326f1576c6cfc85bb43e10147e145638ef6cfd737dffa932a236fd1ce3db","title":"Introduce DeploymentId value object to replace string primitives","description":"Replace string primitive for deployment_id with a proper value object for type safety and validation.\n\nCurrently deployment_id is typed as `str` in multiple locations across instances/ and pytest_plugin/, allowing any string to be used as a deployment identifier.\n\nA DeploymentId value object would:\n- Ensure valid identifier format\n- Prevent confusion with server_id\n- Make deployment tracking more explicit\n- Add validation at construction time","design":"1. Create DeploymentId class in armadillo/core/value_objects.py\n2. Update type hints in instances/manager.py, orchestrator.py, pytest_plugin/plugin.py\n3. Update factory methods and constructors\n4. Update registry/lookup logic\n5. Update all tests\n\nImplementation notes:\n- DeploymentId validates non-empty alphanumeric identifiers\n- Implements __eq__, __hash__ for use in registries\n- Provides .value property for string access","acceptance_criteria":"- DeploymentId class implemented with validation\n- All type hints updated from `str` to `DeploymentId`\n- All construction sites validated\n- All registry/lookup logic updated\n- All tests passing\n- No string primitives used for deployment identifiers","status":"closed","priority":4,"issue_type":"chore","created_at":"2025-10-26T11:29:09.04921488+01:00","updated_at":"2025-11-04T16:18:29.47076424+01:00","closed_at":"2025-11-04T16:18:29.47076424+01:00","source_repo":"."}
{"id":"armadillo-14","content_hash":"cdbe42704d9d2bcb4298db35fc2ca882c43c3bfa462a67540b12adb87ecc5ff9","title":"Naming consistency audit and cleanup","description":"Perform final pass to ensure naming conventions are consistent across the codebase.\n\nAudit for:\n- Private methods (_method) that should be public or vice versa\n- Inconsistent naming patterns (get_ vs fetch_, check_ vs verify_)\n- Parameter names that signal intent incorrectly (e.g., _timeout suggesting unused)\n- Methods that violate single responsibility in their naming\n- Public APIs that expose implementation details","design":"1. Systematic audit by module: core/*, instances/*, pytest_plugin/*, utils/*\n2. Document naming conventions in NAMING_CONVENTIONS.md:\n   - When to use get_ vs fetch_ (get = sync lookup, fetch = may involve I/O)\n   - When to use check_ vs verify_ (check = boolean return, verify = raises on failure)\n   - When methods should be private (internal helpers, not part of public API)\n   - Parameter naming (leading underscore only when actually unused)\n3. Create naming consistency guide document\n4. Refactor any violations found\n5. Consider creating linter rules for common patterns","acceptance_criteria":"- Naming conventions documented\n- All inconsistencies identified and catalogued\n- Critical violations fixed\n- Public API naming is consistent and intuitive\n- Private methods are appropriately scoped\n- All tests passing","status":"closed","priority":4,"issue_type":"chore","created_at":"2025-10-26T11:29:12.191103546+01:00","updated_at":"2025-10-30T16:00:22.630191705+01:00","closed_at":"2025-10-30T16:00:22.630191705+01:00","source_repo":"."}
{"id":"armadillo-16","content_hash":"1fb2905cd39b298a4f37a4e9d5ef7f942f018e943799d52ac4e1883312e5b0f5","title":"Add pytest-timeout as runtime dependency","description":"Add pytest-timeout to pyproject.toml dependencies to enable per-test timeout enforcement via SIGALRM on Linux.\n\nChanges:\n- pyproject.toml: add pytest-timeout~=2.2 to dependencies\n- Verify it works with current pytest version\n- Document in README that per-test timeouts use signal mode (Linux-only, interrupts blocking I/O)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-28T14:17:50.383864841+01:00","updated_at":"2025-10-28T16:08:00.734614021+01:00","closed_at":"2025-10-28T16:08:00.734614021+01:00","source_repo":"."}
{"id":"armadillo-17","content_hash":"a5b84e2229aa40e775f652a66a77dbf4d9b48893bc60bb6bec3512db48d140d8","title":"CLI: add --global-timeout flag and map to ArmadilloConfig.test_timeout","description":"Introduce --global-timeout CLI flag to set the entire test session deadline (currently hard-coded 900s default in ArmadilloConfig).\n\nChanges:\n- armadillo/cli/commands/test.py: add global_timeout field to TestRunOptions\n- Pass global_timeout to ArmadilloConfig constructor as test_timeout\n- Update help text to clarify: --timeout is per-test, --global-timeout is for entire session\n- Update README with both flags and their defaults","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-28T14:17:55.065938821+01:00","updated_at":"2025-10-28T16:09:23.583342919+01:00","closed_at":"2025-10-28T16:09:23.583342919+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-17","depends_on_id":"armadillo-16","type":"blocks","created_at":"2025-10-28T14:17:55.079414794+01:00","created_by":"daemon"}]}
{"id":"armadillo-18","content_hash":"454cdd2801dd0cb7083572407268077b0c54ae0170fdcc0337dbb75d6905845c","title":"Implement output-idle timeout for pytest subprocess","description":"Add --output-idle-timeout flag to kill pytest if it produces no stdout/stderr for N seconds (detect hung tests that neither timeout nor fail).\n\nDesign:\n- CLI: add output_idle_timeout field (default: None = disabled)\n- armadillo/cli/commands/test.py: instead of subprocess.run, use Popen with line-by-line tee + idle tracking\n- Track last_output_time; if time.monotonic() - last_output_time \u003e threshold, escalate: SIGTERM, wait 5s, SIGKILL\n- Log clear message when idle timeout triggers\n- Update README with flag and recommended value (e.g., 300s for CI)\n\nConstraints:\n- Must not buffer output (line-buffered tee)\n- Must not interfere with user-visible output formatting","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-28T14:18:00.560320187+01:00","updated_at":"2025-10-28T16:11:38.245785559+01:00","closed_at":"2025-10-28T16:11:38.245785559+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-18","depends_on_id":"armadillo-17","type":"blocks","created_at":"2025-10-28T14:18:00.574223136+01:00","created_by":"daemon"}]}
{"id":"armadillo-19","content_hash":"171acfd061e5fd6867566bce4010e2aed1e0d3917cc4aedc4ce46258373ea248","title":"Add integration tests for all three timeout modes","description":"Write framework tests to verify timeout enforcement works correctly for per-test, global, and output-idle timeouts.\n\nTest cases:\n- Per-test timeout: test that sleeps longer than --timeout is killed with pytest-timeout traceback\n- Global timeout: long test suite exceeding --global-timeout triggers ArmadilloConfig.test_timeout enforcement\n- Output-idle timeout: test that hangs without output for longer than --output-idle-timeout is terminated\n- No false positives: tests that finish cleanly within all thresholds pass\n\nLocation: framework_tests/integration/test_timeouts.py","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-28T14:18:05.696439941+01:00","updated_at":"2025-10-28T16:14:16.342232992+01:00","closed_at":"2025-10-28T16:14:16.342232992+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-19","depends_on_id":"armadillo-18","type":"blocks","created_at":"2025-10-28T14:18:05.709716009+01:00","created_by":"daemon"}]}
{"id":"armadillo-1p5","content_hash":"aa5e5d67ab009779753d457984e4b1b65e9b2510a371ea26be0168a0aa598733","title":"Replace 'assert x is not None' checks with meaningful assertions","description":"45 remaining 'assert x is not None' checks across test files. These are weak assertions that add little value. Replace with assertions that test actual behavior or remove if the next line would crash anyway.","acceptance_criteria":"Audit all 'is not None' assertions; replace with behavioral checks or remove redundant ones; document decision for keepers","status":"open","priority":3,"issue_type":"chore","created_at":"2025-11-14T12:36:48.945977572+01:00","updated_at":"2025-11-14T12:36:48.945977572+01:00","source_repo":"."}
{"id":"armadillo-21","content_hash":"d4708704dc1734b68c6412f4e7b6ec00c0f36d8ee04f4121dbfbe6677e8feaa3","title":"Print temp directory path at test run start","description":"Display the temporary directory path being used for the test run at startup. This helps with debugging and manual inspection of server data/logs during or after test execution.\n\nCurrently temp_dir is created but path not prominently displayed to user.","design":"Implementation:\n1. In CLI test command (cli/commands/test.py), after config initialization\n2. Print prominent message: \"Test artifacts directory: /path/to/temp/dir\"\n3. Use rich formatting for visibility (bold or highlighted)\n4. Also log at INFO level for structured logs\n\nLocation: Right after test session starts, before server deployment","acceptance_criteria":"- Temp directory path printed at test run start\n- Visible in both compact and verbose modes\n- Path is absolute and easy to copy-paste\n- Logged in structured logs","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-31T14:10:46.64407527+01:00","updated_at":"2025-11-02T11:35:43.057185786+01:00","closed_at":"2025-11-02T11:35:43.057185786+01:00","source_repo":"."}
{"id":"armadillo-22","content_hash":"9d7a24b96d56202489532d38a984ea8a9f4288a6375369f48201cfc3a3e162e6","title":"Add --keep-temp-dir option to preserve test artifacts","description":"Add CLI option to control temp directory cleanup behavior. By default, cleanup temp directory on successful test runs. Allow users to preserve artifacts for inspection.\n\nUse cases:\n- Debugging: inspect server data/logs even after successful tests\n- Development: examine state between test runs\n- CI: preserve artifacts for upload before cleanup","design":"Implementation:\n1. Add --keep-temp-dir flag to CLI (default: False)\n2. Add ARMADILLO_KEEP_TEMP_DIR environment variable\n3. Current behavior already has keep_instances_on_failure - extend this\n4. Cleanup logic:\n   - On failure: always keep (existing behavior)\n   - On success with --keep-temp-dir: keep\n   - On success without flag: cleanup temp_dir\n5. Print message when keeping: \"Preserving test artifacts at: /path\"\n\nFiles:\n- cli/commands/test.py: add flag\n- core/config.py: add keep_temp_dir field\n- pytest_plugin/plugin.py: implement cleanup logic in session finish","acceptance_criteria":"- --keep-temp-dir CLI flag works\n- Environment variable ARMADILLO_KEEP_TEMP_DIR works\n- Temp dir cleaned up on success by default\n- Temp dir preserved on success when flag set\n- Temp dir always preserved on failure\n- Clear message when preserving artifacts","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-31T14:10:53.022792506+01:00","updated_at":"2025-11-04T14:43:55.743578564+01:00","closed_at":"2025-11-04T14:43:55.743578564+01:00","source_repo":"."}
{"id":"armadillo-23","content_hash":"ce3324870589588d86a6370f73ba946e16b330863e9e3fd3fd35aaadb4e999e6","title":"Add sanitizer support (ASan, UBSan, TSan)","description":"Add support for running ArangoDB with sanitizers (AddressSanitizer, UndefinedBehaviorSanitizer, ThreadSanitizer).\n\nRequirements:\n- Detect if arangod binary is built with sanitizers\n- Set up proper environment variables for sanitizer runtime\n- Configure sanitizer output to temp directory\n- Detect and report sanitizer errors in test results\n- Parse sanitizer reports for failures","design":"Implementation:\n1. Detection (build_detection.py):\n   - Check arangod binary for sanitizer symbols/flags\n   - Detect which sanitizers are enabled\n   \n2. Environment setup (instances/server.py or command_builder.py):\n   - ASAN_OPTIONS: log_path={temp_dir}/asan.log, detect_leaks=1, etc.\n   - UBSAN_OPTIONS: log_path={temp_dir}/ubsan.log, print_stacktrace=1\n   - TSAN_OPTIONS: log_path={temp_dir}/tsan.log, history_size=7\n   \n3. Report collection (results/collector.py or new sanitizer_analyzer.py):\n   - Check for sanitizer log files in temp_dir after test\n   - Parse reports for errors\n   - Mark test as failed if sanitizer errors found\n   - Include sanitizer output in test failure details\n   \n4. CLI support:\n   - Auto-detect or --sanitizer flag\n   - --sanitizer-options for custom runtime options\n\nCommon sanitizer options:\n- ASan: detect_leaks, detect_stack_use_after_return, check_initialization_order\n- UBSan: print_stacktrace\n- TSan: history_size, io_sync","acceptance_criteria":"- Sanitizer detection works for ASan/UBSan/TSan\n- Environment variables correctly set for arangod process\n- Sanitizer logs written to temp directory\n- Sanitizer errors detected and reported\n- Tests fail when sanitizer finds issues\n- Sanitizer output included in failure details","status":"open","priority":1,"issue_type":"feature","created_at":"2025-10-31T14:10:58.805695063+01:00","updated_at":"2025-10-31T14:10:58.805695063+01:00","source_repo":"."}
{"id":"armadillo-24","content_hash":"31689e90aeb27b6f33c273b4b02c7b82b731eac4b01e5d0b2372f3fd8d61568a","title":"Add option to abort arangod with SIGABRT for coredump generation","description":"Add ability to send SIGABRT to arangod processes instead of graceful shutdown (SIGTERM). This triggers coredump generation when core dumps are enabled, useful for debugging.\n\nUse cases:\n- Post-mortem debugging: generate coredump even from successful tests\n- CI debugging: capture full server state for analysis\n- Development: inspect server internals after specific test scenarios","design":"Implementation:\n1. Add --coredump-on-exit flag (or similar name)\n2. Add per-test marker: @pytest.mark.coredump_on_exit\n3. Modify ProcessSupervisor.stop() to accept signal parameter\n4. When enabled, send SIGABRT instead of SIGTERM\n5. Wait for coredump generation (may take time)\n6. Detect if coredumps actually generated (check /proc/sys/kernel/core_pattern)\n\nFiles:\n- core/process.py: add signal parameter to stop methods\n- instances/server.py: pass signal based on config/marker\n- core/config.py: add coredump_on_exit option\n- pytest_plugin/plugin.py: check for marker, configure accordingly\n\nSafety:\n- Only use SIGABRT during shutdown, not during normal operation\n- Document that this requires proper core dump configuration\n- Warn if core dumps not enabled in system","acceptance_criteria":"- --coredump-on-exit CLI flag works\n- Per-test marker @pytest.mark.coredump_on_exit works\n- SIGABRT sent to arangod on shutdown when enabled\n- Coredumps generated (if system configured)\n- Warning shown if coredumps not enabled in system\n- Documentation updated with core dump setup instructions","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-31T14:11:04.519385205+01:00","updated_at":"2025-10-31T14:11:04.519385205+01:00","source_repo":"."}
{"id":"armadillo-25","content_hash":"c927a2f713475eec8c87b1ae46da8bcaa5524f33ee13f85e9a90ef1577e1033a","title":"Implement coredump analysis with gdb/lldb","description":"Add automatic coredump analysis when server crashes or SIGABRT is sent. Extract backtrace, thread information, and variable state from coredump files.\n\nThis helps with:\n- Automatic crash diagnosis in CI\n- Rich failure reports with stack traces\n- Post-mortem debugging without manual gdb session","design":"Implementation:\n1. Coredump detection (utils/coredump.py or results/crash_analyzer.py):\n   - Find coredumps in temp_dir or system location\n   - Match coredump to arangod process (PID, timestamp)\n   \n2. Debugger abstraction:\n   - Support both gdb and lldb (configurable)\n   - --debugger flag: gdb (default), lldb, or auto-detect\n   - Abstract interface: DebuggerInterface with GdbDebugger and LldbDebugger implementations\n   \n3. GDB automation:\n   - Run: gdb -batch -ex \"commands\" core.file binary\n   - Commands: 'bt full', 'info threads', 'thread apply all bt'\n   \n4. LLDB automation:\n   - Run: lldb -c core.file -b -o \"commands\" binary\n   - Commands: 'bt all', 'thread backtrace all', 'thread list'\n   \n5. Report generation:\n   - Parse gdb/lldb output (different formats)\n   - Format as structured data\n   - Include in test failure details\n   - Save as separate artifact (coredump_analysis.txt)\n   \n6. Integration:\n   - Run analysis in pytest_plugin after test/session\n   - Detect crashes via ProcessSupervisor crash detection\n   - Attach analysis to failed test result\n   \n7. Configuration:\n   - --analyze-coredumps flag (default: true if coredumps found)\n   - --debugger {gdb,lldb,auto} flag\n   - Auto-detect: check which debugger is available\n   - Gracefully handle missing debugger or debug symbols\n\nFiles:\n- utils/coredump.py: coredump detection\n- utils/debugger.py: debugger abstraction (GdbDebugger, LldbDebugger)\n- results/crash_analyzer.py: analysis orchestration\n- pytest_plugin/plugin.py: integrate with test lifecycle","acceptance_criteria":"- Coredumps automatically detected\n- Both gdb and lldb supported\n- --debugger flag for selection (gdb/lldb/auto)\n- Backtrace extracted and formatted from both debuggers\n- Thread information included\n- Analysis attached to test failure report\n- Graceful handling when debugger missing\n- Works with both crash and SIGABRT coredumps\n- Auto-detection picks available debugger","status":"open","priority":1,"issue_type":"feature","created_at":"2025-10-31T14:11:13.480320649+01:00","updated_at":"2025-10-31T14:12:23.394510934+01:00","source_repo":"."}
{"id":"armadillo-26","content_hash":"71df25a42684386dbe5a9fe006bf4603ffb44de2758819d9b1fd39f026a8f3e7","title":"Implement log-based crash analysis (fallback when no coredump)","description":"When server crashes but no coredump available, analyze log files for crash information. ArangoDB prints stack traces to logs on crash - extract and format these for reporting.\n\nFallback for when:\n- Coredumps disabled in system\n- Coredump not found/matched\n- Quick crash diagnosis without full coredump analysis","design":"Implementation:\n1. Log analysis (utils/log_analyzer.py):\n   - Search arangod logs for crash indicators\n   - Patterns: \"FATAL\", \"Signal\", \"Backtrace:\", stack trace lines\n   - Extract relevant sections around crash\n   \n2. Stack trace extraction:\n   - Parse ArangoDB log format for stack traces\n   - Extract function names, addresses, source locations\n   - Format for readability\n   \n3. Integration:\n   - Run when ProcessSupervisor detects crash\n   - Run as fallback if coredump analysis fails/unavailable\n   - Include in test failure details\n   \n4. Report format:\n   - Extracted stack trace\n   - Signal information\n   - Surrounding log context\n   - Save as crash_analysis_from_logs.txt\n\nFiles:\n- utils/log_analyzer.py: log parsing and crash detection\n- results/crash_analyzer.py: coordinate with coredump analysis\n- pytest_plugin/plugin.py: invoke on crash detection\n\nPriority: Run coredump analysis first if available, fall back to logs","acceptance_criteria":"- Log files searched for crash indicators\n- Stack traces extracted from logs\n- Crash information formatted in report\n- Works as fallback when no coredump\n- Attached to test failure details\n- Handles various ArangoDB log formats","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-31T14:11:22.287130825+01:00","updated_at":"2025-10-31T14:11:22.287130825+01:00","source_repo":"."}
{"id":"armadillo-27","content_hash":"1841d62fc774d327070552d8ceaa43276ebd1bb96afd6c6c536a9fbbc40d596b","title":"Create artifact archive for CI publishing","description":"Add option to create compressed archive of entire temp directory for CI artifact publishing. Archive should include all logs, data, coredumps, analysis reports, and test results.\n\nUse case: CI pipelines need to publish test artifacts for later inspection, especially on failures.","design":"Implementation:\n1. Add --create-archive or --archive-artifacts flag\n2. Archive creation (utils/archive.py or results/archiver.py):\n   - Create tar.gz or zip of temp_dir\n   - Include: server logs, data dirs, coredumps, analysis reports, test results\n   - Exclude: large unnecessary files (database files if needed)\n   - Name: armadillo-artifacts-{timestamp}-{session-id}.tar.gz\n   \n3. Timing:\n   - Create archive at end of test session (pytest_sessionfinish)\n   - Before temp directory cleanup\n   - Even on failures\n   \n4. Location:\n   - Save archive to output_dir or current directory\n   - Print path prominently for CI to find\n   \n5. Optimization:\n   - Option to compress heavily (slower) vs faster compression\n   - Option to exclude specific patterns\n   - Archive only on failure (--archive-on-failure)\n\nFiles:\n- utils/archive.py: compression logic\n- cli/commands/test.py: add flags\n- pytest_plugin/plugin.py: trigger at session end\n- core/config.py: archive options\n\nCI integration:\n- Archive path predictable for CI scripts\n- Exit code preserved even if archive fails","acceptance_criteria":"- --create-archive flag creates tar.gz of temp dir\n- Archive includes all relevant artifacts\n- Archive created before cleanup\n- Archive path printed clearly\n- Works even when tests fail\n- Optional --archive-on-failure variant\n- Reasonable compression (not too slow)\n- Predictable archive naming for CI scripts","status":"open","priority":1,"issue_type":"feature","created_at":"2025-10-31T14:11:32.616597857+01:00","updated_at":"2025-10-31T14:11:32.616597857+01:00","source_repo":"."}
{"id":"armadillo-28","content_hash":"51973be13f4aad99e18e535c01c78c00f6b070ae8f609ca4120920aab32ed078","title":"Refactor pytest plugin to use single shared ApplicationContext","description":"Currently each InstanceManager creates its own ApplicationContext via get_instance_manager(), leading to multiple independent contexts and no single source of truth for session artifacts location.\n\nThis makes it difficult to:\n- Report where test artifacts are stored\n- Share configuration across deployments\n- Track session-level resources\n\nDesign flaw: get_instance_manager() factory creates new context each time, but pytest session should have ONE context shared by all deployments.","design":"Refactor to single ApplicationContext per pytest session:\n\n1. Pytest plugin creates ApplicationContext at session start:\n   - Generate session_id\n   - Create context from framework config\n   - Set session_id on filesystem service\n   - Store as _session_app_context\n\n2. Update get_instance_manager() signature:\n   - Add optional app_context parameter\n   - If provided, use it; if None, create new one (backward compat)\n   \n3. Update pytest plugin deployment methods:\n   - Pass _session_app_context to get_instance_manager()\n   - All session deployments share same context\n   \n4. Clean artifact directory reporting:\n   - Simply use _session_app_context.filesystem.work_dir()\n   - No hasattr checks or private attribute access\n   \n5. Benefits:\n   - Single source of truth for session resources\n   - Clean access to artifacts directory\n   - Shared port allocator (better port management)\n   - Consistent filesystem paths across deployments\n\nFiles:\n- pytest_plugin/plugin.py: create and manage session context\n- instances/manager.py: update get_instance_manager() signature\n- All fixtures: pass session context","acceptance_criteria":"- Pytest plugin creates ONE ApplicationContext per session\n- get_instance_manager() accepts optional app_context parameter\n- All session deployments use shared context\n- Clean access to artifacts directory (no hasattr hacks)\n- Backward compatibility maintained for direct get_instance_manager() calls\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-31T14:26:59.993554441+01:00","updated_at":"2025-11-02T11:35:41.815509316+01:00","closed_at":"2025-11-02T11:35:41.815509316+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-28","depends_on_id":"armadillo-21","type":"blocks","created_at":"2025-10-31T14:27:00.007456459+01:00","created_by":"daemon"}]}
{"id":"armadillo-29","content_hash":"7d2795314138fc269c7cf29cb7272b72540c0110fcc4564032ec4a95657fdb61","title":"Ensure single shared ApplicationContext across entire framework","description":"After armadillo-28, we still had 4 fixtures creating their own ApplicationContext instances instead of using the shared session context:\n1. arango_single_server (session)\n2. arango_single_server_function (function)  \n3. arango_cluster (session)\n4. arango_cluster_function (function)\n\nThis caused:\n- Multiple session IDs (breaks artifact directory tracking)\n- Separate port allocators (possible port conflicts)\n- Inconsistent configuration\n- Wasted memory overhead\n\nGoal: ONE ApplicationContext per pytest session, period.","design":"Update all 4 fixtures to use _plugin._session_app_context:\n\nBefore:\n```python\napp_context = ApplicationContext.create(get_config())\nmanager = get_instance_manager(deployment_id, app_context)\n```\n\nAfter:\n```python\nmanager = get_instance_manager(deployment_id, _plugin._session_app_context)\n```\n\nRemove unnecessary imports of ApplicationContext and get_config from fixtures.\n\nVerified production code has only ONE place creating ApplicationContext: pytest_sessionstart hook.","acceptance_criteria":"- All 4 fixtures use _plugin._session_app_context\n- No ApplicationContext.create() calls except in pytest_sessionstart\n- All 499+ tests passing\n- Single session ID for all deployments in a test run\n- Shared port allocator across all fixtures","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-02T11:47:54.643637229+01:00","updated_at":"2025-11-02T11:47:59.480529524+01:00","closed_at":"2025-11-02T11:47:59.480529524+01:00","source_repo":"."}
{"id":"armadillo-29h","content_hash":"1c173674f20c96b5186375e0085c5e0ff75eb15bbd746392e6c29b17edaba831","title":"Add health check retry logic tests","description":"Test health check retry behavior: exponential backoff works correctly, max retries respected, timeout handling, transient failures vs permanent failures.","acceptance_criteria":"Tests verify: retry count accurate; exponential backoff timing correct; timeouts handled; distinguishes transient from permanent failures","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-14T12:35:42.847781718+01:00","updated_at":"2025-11-14T12:35:42.847781718+01:00","source_repo":"."}
{"id":"armadillo-30","content_hash":"924856adb3e529c7c575e47568699f2a5ac4f21547dd056b5618e2ef3afc96e5","title":"Rename process_id to server_id for clarity","description":"Current naming is misleading: 'process_id' throughout the codebase refers to logical server identifiers like \"agent_0\", \"dbserver_1\", \"coordinator_0\" - NOT OS-level process IDs (PIDs).\n\nThis causes confusion:\n- process_id in ProcessSupervisor: \"agent_0\" (string)\n- ProcessInfo.pid: 12345 (int) - actual OS PID\n- crash_state keys: \"agent_0\" (string)\n- ArangoServer.server_id: \"agent_0\" (string)\n\nThe terminology mixes logical identifiers with process management, making code harder to understand.","design":"Systematic rename of process_id to server_id where it refers to logical identifiers:\n\nFiles to update:\n- core/process.py: ProcessSupervisor methods and attributes\n  - start(server_id, ...) instead of start(process_id, ...)\n  - _crash_state: Dict[str, CrashInfo] keys\n  - _processes, _process_info dictionary keys\n  - get_crash_state(server_id) parameter\n  - Log messages and error messages\n  \n- instances/server.py: All call sites to start_supervised_process()\n- instances/manager.py: Any references\n- instances/orchestrator.py: Any references\n- pytest_plugin/plugin.py: Crash state iteration\n\nKeep OS PID references as 'pid' (ProcessInfo.pid, process.pid, etc.)\n\nThis is pure refactoring with no functional changes.","acceptance_criteria":"- All 'process_id' parameters renamed to 'server_id' where referring to logical identifiers\n- OS PIDs still called 'pid'\n- Dictionary keys updated (crash_state, processes, process_info)\n- Log messages updated for consistency\n- All tests passing\n- No functional changes, pure refactoring","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-02T18:40:43.160182264+01:00","updated_at":"2025-11-02T18:48:47.580795907+01:00","closed_at":"2025-11-02T18:48:47.580795907+01:00","source_repo":"."}
{"id":"armadillo-31","content_hash":"c125074902ec0bfb5794da45c66243f2d8b4eb7c8aae4458f172221c7b6d3121","title":"Introduce ServerId value object for type safety","description":"After renaming process_id to server_id (armadillo-30), introduce a ServerId value object to replace string primitives with proper type safety.\n\nCurrently server_id is typed as `str` in ~27+ locations across the codebase (instances/server.py, manager.py, server_registry.py, deployment_planner.py, core/process.py, etc.), leading to potential bugs where any string can be passed.\n\nBenefits:\n- Type safety: Can't confuse server IDs with arbitrary strings\n- Validation: Prevent empty/invalid identifiers at construction time\n- Explicit relationship between logical ID, role, and OS PID\n- Can add helper methods (is_agent(), is_coordinator(), is_running())\n- Self-documenting code\n- Catches errors at construction time rather than runtime\n\nTrade-offs:\n- More complexity than plain strings\n- Need to update Dict[str, X] to Dict[ServerId, X]\n- Serialization handling required\n- Larger refactoring effort (~27+ locations to update)\n\nThis should be evaluated based on:\n- How often we need both name + PID together\n- Whether type confusion is causing actual bugs\n- Team preference for value objects vs primitives","design":"Two implementation options:\n\n## Option 1: Full Value Object (Recommended)\n\nCreate armadillo/core/value_objects.py with comprehensive ServerId class:\n\n```python\n@dataclass(frozen=True)\nclass ServerId:\n    \\\"\\\"\\\"Immutable identifier for a server instance.\\\"\\\"\\\"\n    name: str  # \"agent_0\", \"dbserver_1\"\n    role: ServerRole\n    pid: Optional[int] = None  # OS process ID when running\n    \n    def __post_init__(self):\n        if not self.name or not self.name.strip():\n            raise ValueError(\"ServerId name cannot be empty\")\n        if not self.name.replace(\"_\", \"\").isalnum():\n            raise ValueError(f\"ServerId must be alphanumeric: {self.name}\")\n    \n    def __str__(self) -\u003e str:\n        return self.name\n    \n    def __hash__(self) -\u003e int:\n        return hash(self.name)\n    \n    def __eq__(self, other) -\u003e bool:\n        if isinstance(other, ServerId):\n            return self.name == other.name\n        return False\n    \n    @property\n    def value(self) -\u003e str:\n        \\\"\\\"\\\"Get string value for legacy code.\\\"\\\"\\\"\n        return self.name\n    \n    def is_agent(self) -\u003e bool:\n        return self.role == ServerRole.AGENT\n    \n    def is_coordinator(self) -\u003e bool:\n        return self.role == ServerRole.COORDINATOR\n    \n    def is_dbserver(self) -\u003e bool:\n        return self.role == ServerRole.DBSERVER\n    \n    def is_running(self) -\u003e bool:\n        return self.pid is not None\n```\n\n## Option 2: Lightweight NewType (Alternative)\n\n```python\nfrom typing import NewType\nServerId = NewType('ServerId', str)\n```\n\nThis gives some type checking without full value object complexity, but no validation or helper methods.\n\n## Implementation Plan\n\nFiles to update (~27 locations):\n1. Create core/value_objects.py with ServerId class\n2. Update type hints:\n   - instances/server.py\n   - instances/manager.py\n   - instances/server_registry.py\n   - instances/deployment_planner.py\n   - core/process.py (ProcessSupervisor)\n3. Update factory methods to construct ServerId objects\n4. Update comparison logic: `server_id == \"foo\"` → `server_id.value == \"foo\"` or `str(server_id) == \"foo\"`\n5. Update dictionaries: Dict[str, X] → Dict[ServerId, X]\n6. Update serialization in results collector\n7. Add __str__ support for logging (already in design)\n8. Update all tests to use ServerId constructor","acceptance_criteria":"- ServerId class or NewType implemented\n- ProcessSupervisor uses ServerId consistently\n- Dictionaries keyed by ServerId\n- Serialization/deserialization working\n- Helper methods implemented (if full class)\n- All tests updated and passing\n- Performance impact evaluated (dict lookups, hashing)\n\nSuccess criteria:\n- Type checker catches mixing server IDs with other strings\n- Code is more self-documenting\n- No significant performance regression","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-11-02T18:43:24.927591633+01:00","updated_at":"2025-11-03T18:51:25.130843767+01:00","closed_at":"2025-11-03T18:51:25.130843767+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-31","depends_on_id":"armadillo-30","type":"blocks","created_at":"2025-11-02T18:43:24.94188379+01:00","created_by":"daemon"}]}
{"id":"armadillo-32","content_hash":"32e4183682dc0c98af27a91f689e70dab5e102cf0805cf7722c8443775eb022d","title":"Create ServerId and ServerContext value objects","description":"Create the foundational value objects for server identification and context.\n\nThis is Phase 1 of the ServerId refactoring (armadillo-31).\n\nFiles to create:\n- armadillo/core/value_objects.py (new file)\n\nIncludes:\n1. ServerId: Pure immutable identifier with validation\n2. ServerContext: Enriched snapshot for logging/diagnostics with PID\n3. Unit tests for both classes","design":"ServerId implementation:\n- @dataclass(frozen=True) with single field: value: str\n- Validation: non-empty, alphanumeric with _ or -\n- __str__, __hash__, __eq__ for use as dict keys\n- Simple string wrapper, no role/pid coupling\n\nServerContext implementation:\n- Fields: server_id: ServerId, role: ServerRole, pid: Optional[int], port: Optional[int]\n- __str__ formats as \"agent_0[pid:12345]\" or \"agent_0[not running]\"\n- Helper methods: is_running(), is_agent(), is_coordinator(), is_dbserver()\n- Immutable snapshot for diagnostics\n\nExport both from armadillo/core/__init__.py","acceptance_criteria":"- armadillo/core/value_objects.py created\n- ServerId validates input correctly\n- ServerId works as dict key (hashable, comparable)\n- ServerContext formats properly for logs\n- Both exported from core module\n- Unit tests in framework_tests/unit/test_core_value_objects.py\n- All existing tests still pass","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:18.84064336+01:00","updated_at":"2025-11-03T16:29:04.067171297+01:00","closed_at":"2025-11-03T16:29:04.067171297+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-32","depends_on_id":"armadillo-31","type":"parent-child","created_at":"2025-11-03T16:17:08.106305135+01:00","created_by":"daemon"}]}
{"id":"armadillo-33","content_hash":"49757a205964a085643720c6928224065c25e0dae3bc18d4a5cff623e6aa8cb6","title":"Update ProcessSupervisor to use ServerId","description":"Refactor core/process.py ProcessSupervisor to use ServerId instead of string primitives.\n\nThis is Phase 2 of the ServerId refactoring (armadillo-31).\n\nChanges:\n- All internal dictionaries keyed by ServerId\n- All method signatures accept ServerId\n- Add get_server_context() method\n- Update logging to use ServerContext where beneficial","design":"Update ProcessSupervisor:\n1. Change dictionary types:\n   - _processes: Dict[ServerId, subprocess.Popen]\n   - _process_info: Dict[ServerId, ProcessInfo]\n   - _monitoring_threads: Dict[ServerId, threading.Thread]\n   - _streaming_threads: Dict[ServerId, threading.Thread]\n   - _crash_state: Dict[ServerId, CrashInfo]\n\n2. Update method signatures to accept ServerId:\n   - start(server_id: ServerId, ...)\n   - stop(server_id: ServerId, ...)\n   - All internal methods\n\n3. Add: get_server_context(server_id: ServerId, role: ServerRole) -\u003e Optional[ServerContext]\n   - Returns snapshot with current PID if running\n\n4. Update logging to show PID via ServerContext\n\nFile: armadillo/core/process.py","acceptance_criteria":"- All ProcessSupervisor dictionaries use ServerId keys\n- All method signatures updated\n- get_server_context() implemented\n- Crash state tracking uses ServerId\n- Logs show server_id[pid:X] format\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:24.991386094+01:00","updated_at":"2025-11-03T16:48:58.331053209+01:00","closed_at":"2025-11-03T16:48:58.331053209+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-33","depends_on_id":"armadillo-32","type":"blocks","created_at":"2025-11-03T16:16:25.008951611+01:00","created_by":"daemon"}]}
{"id":"armadillo-34","content_hash":"41af858ea1442d3b72fb98fe741746a8a202c143c108281b31c7eb76a6b53349","title":"Update logging functions to support ServerContext","description":"Enhance armadillo/core/log.py to support enriched logging with ServerContext while maintaining backward compatibility.\n\nThis enables logs to show both server_id and PID together: \"Server agent_0[pid:12345] started\"","design":"Update log_server_event():\n1. Add overload accepting ServerContext parameter\n2. When ServerContext provided:\n   - Use str(context) in log message → \"agent_0[pid:12345]\"\n   - Add structured fields: server_id, pid, role to extra dict\n3. Keep backward compatible version accepting server_id: str\n4. Update docstrings with examples\n\nFile: armadillo/core/log.py","acceptance_criteria":"- log_server_event() accepts ServerContext\n- Logs format as \"Server agent_0[pid:12345] event\"\n- Structured logging includes pid field\n- Backward compatibility maintained\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:28.791905323+01:00","updated_at":"2025-11-03T17:09:57.405945318+01:00","closed_at":"2025-11-03T17:09:57.405945318+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-34","depends_on_id":"armadillo-32","type":"blocks","created_at":"2025-11-03T16:16:28.810680426+01:00","created_by":"daemon"}]}
{"id":"armadillo-34f","content_hash":"f7a572a1ab54f9dfe07e628eef6892959552272c080e98fbef61891fdd50417b","title":"Add concurrent deployment tests","description":"Test concurrent deployment scenarios: multiple deployments in parallel, race conditions in port allocation, concurrent shutdown requests. Verify thread safety and resource isolation.","acceptance_criteria":"Tests verify: parallel deployments don't conflict; port allocation is thread-safe; concurrent shutdowns handled correctly; no race conditions in resource management","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-14T12:35:41.847791322+01:00","updated_at":"2025-11-14T12:35:41.847791322+01:00","source_repo":"."}
{"id":"armadillo-35","content_hash":"00921ef9af2bed097a7e8e353819215d61dd6ac63830017ae5a450e980af0174","title":"Update ArangoServer and ServerFactory to use ServerId","description":"Refactor server instance management to use ServerId primitives.\n\nThis is Phase 3 of the ServerId refactoring (armadillo-31).\n\nFiles:\n- instances/server.py: ArangoServer class\n- instances/server_factory.py: ServerFactory classes\n- instances/server_registry.py: ServerRegistry","design":"instances/server.py:\n- ArangoServer.__init__: server_id: ServerId parameter\n- Factory methods accept str | ServerId, normalize to ServerId\n- Add get_context() -\u003e ServerContext method\n- Update all logging to use ServerContext\n\ninstances/server_factory.py:\n- _generate_server_id() returns ServerId(f\"agent_{i}\")\n- create_server_instances() returns Dict[ServerId, ArangoServer]\n- Update all type hints\n\ninstances/server_registry.py:\n- _servers: Dict[ServerId, ArangoServer]\n- All methods accept/return ServerId\n- Update type hints throughout","acceptance_criteria":"- ArangoServer uses ServerId internally\n- Factory generates ServerId objects\n- Registry keyed by ServerId\n- All type hints updated\n- Logging shows PID via ServerContext\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:33.086968088+01:00","updated_at":"2025-11-03T17:23:28.702837153+01:00","closed_at":"2025-11-03T17:23:28.702837153+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-35","depends_on_id":"armadillo-33","type":"blocks","created_at":"2025-11-03T16:16:33.105345381+01:00","created_by":"daemon"},{"issue_id":"armadillo-35","depends_on_id":"armadillo-34","type":"blocks","created_at":"2025-11-03T16:16:33.113117145+01:00","created_by":"daemon"}]}
{"id":"armadillo-36","content_hash":"2adf3c7121db34bbf0d6ebf5f638d6b354be7b439f37236353e2322d3a3f0a27","title":"Update deployment orchestration to use ServerId","description":"Update deployment planning and orchestration components to use ServerId.\n\nThis is Phase 4 of the ServerId refactoring (armadillo-31).\n\nFiles:\n- instances/deployment_plan.py\n- instances/deployment_planner.py\n- instances/deployment_orchestrator.py\n- instances/manager.py","design":"deployment_plan.py:\n- create_single_server_plan(server_id: str | ServerId, ...) \n- Normalize to ServerId internally\n\ndeployment_orchestrator.py:\n- _startup_order: List[ServerId]\n- Update all method signatures\n- Use ServerContext for error reporting\n\nmanager.py:\n- Review all server_id usage\n- Update any method signatures\n- Ensure consistency","acceptance_criteria":"- Deployment planning uses ServerId\n- Orchestrator tracks order with ServerId\n- Manager methods accept ServerId\n- Error messages include PID via ServerContext\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:37.26618285+01:00","updated_at":"2025-11-03T17:29:54.066668929+01:00","closed_at":"2025-11-03T17:29:54.066668929+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-36","depends_on_id":"armadillo-35","type":"blocks","created_at":"2025-11-03T16:16:37.283480354+01:00","created_by":"daemon"}]}
{"id":"armadillo-37","content_hash":"0dba0d1820aaeaac6345bc65839c5843c914faa973cf6682329d856165b2f3b4","title":"Update health checking and command building for ServerId","description":"Update remaining instance management components to use ServerId.\n\nThis is Phase 5 of the ServerId refactoring (armadillo-31).\n\nFiles:\n- instances/health_checker.py\n- instances/cluster_bootstrapper.py\n- instances/command_builder.py","design":"health_checker.py:\n- Update method signatures to accept ServerId\n- Internal HTTP checks still use string URLs\n\ncluster_bootstrapper.py:\n- Update any server_id references\n- Ensure consistency with ServerId\n\ncommand_builder.py:\n- Review ServerCommandParams\n- Update logging to use ServerId\n- Keep command generation working with strings","acceptance_criteria":"- Health checker accepts ServerId\n- Cluster bootstrapper uses ServerId\n- Command builder handles ServerId\n- All components consistent\n- All tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:41.524768908+01:00","updated_at":"2025-11-03T18:19:33.256478643+01:00","closed_at":"2025-11-03T18:19:33.256478643+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-37","depends_on_id":"armadillo-36","type":"blocks","created_at":"2025-11-03T16:16:41.537179857+01:00","created_by":"daemon"}]}
{"id":"armadillo-38","content_hash":"b35ae8918f34d24cac4c3e85fea141d9aa6a9ab65624c0306ce7e9770dfd7b27","title":"Update pytest plugin and results serialization for ServerId","description":"Handle ServerId in pytest integration and result collection, maintaining JSON backward compatibility.\n\nThis is Phase 6-7 of the ServerId refactoring (armadillo-31).\n\nFiles:\n- pytest_plugin/plugin.py\n- results/collector.py (and related)","design":"pytest_plugin/plugin.py:\n- Update crash state iteration: Dict[ServerId, CrashInfo]\n- Convert ServerId to string for pytest reporting\n- Ensure fixture compatibility\n\nresults/collector.py:\n- Serialize ServerId → str(server_id) for JSON\n- Keep JSON format: {\"server_id\": \"agent_0\"} (backward compatible)\n- No change to external schema\n- Add tests for serialization roundtrip","acceptance_criteria":"- Pytest plugin handles ServerId\n- Crash reporting works correctly\n- JSON serialization maintains schema\n- Backward compatibility verified\n- All integration tests passing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:47.457158657+01:00","updated_at":"2025-11-03T18:50:53.976818334+01:00","closed_at":"2025-11-03T18:50:53.976818334+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-38","depends_on_id":"armadillo-37","type":"blocks","created_at":"2025-11-03T16:16:47.477087689+01:00","created_by":"daemon"}]}
{"id":"armadillo-39","content_hash":"8927b4c77d0ac6208da0144b08676915efe2981a09c4b528f819d29059983f79","title":"Update all framework tests to use ServerId","description":"Update framework unit and integration tests to construct and use ServerId objects correctly.\n\nFinal phase of ServerId refactoring (armadillo-31).\n\nScope:\n- framework_tests/unit/test_core_*.py\n- framework_tests/unit/test_instances_*.py\n- Any other test files using server_id","design":"Test updates:\n1. Update server_id construction to use ServerId()\n2. Test ServerId as dict keys\n3. Update assertions for ServerId equality\n4. Ensure mock objects return ServerId\n5. Add regression tests for string/ServerId confusion\n\nRun full test suite to verify:\n- All 499+ tests passing\n- Type checking with mypy passes\n- No string primitives used for server IDs","acceptance_criteria":"- All unit tests use ServerId\n- All integration tests pass\n- Type checker (mypy) validates ServerId usage\n- No regressions introduced\n- Performance benchmarks show negligible impact\n- Documentation updated if needed","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-03T16:16:53.202081189+01:00","updated_at":"2025-11-03T18:56:47.970600035+01:00","closed_at":"2025-11-03T18:56:47.970600035+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-39","depends_on_id":"armadillo-38","type":"blocks","created_at":"2025-11-03T16:16:53.212003543+01:00","created_by":"daemon"}]}
{"id":"armadillo-40","content_hash":"0bd813cf611da548097f6760864fc530782ae0e19c70e9661b792eecf85d9699","title":"Check server exit codes after test completion","description":"When servers are shut down after tests complete, we need to check their exit codes. Non-zero exit codes (especially from sanitizers) should fail the test run even if all test cases passed.\n\nCurrently:\n- Exit codes captured during crashes (unexpected termination)\n- Exit codes NOT captured during intentional shutdown\n- No post-test server health validation\n\nNeed to:\n1. Capture exit codes when ProcessSupervisor.stop() completes\n2. Store exit codes for all stopped servers\n3. Check exit codes in pytest_sessionfinish\n4. Mark test run as failed if any exit code != 0\n5. Report server health issues in both JSON and JUnit XML\n\nRelated to sanitizer integration - ASAN/UBSAN/TSAN report issues via exit codes.","design":"## Exit Code Capture\n- Modify ProcessSupervisor.stop() to return exit code\n- Store exit codes in _exit_codes: Dict[ServerId, int]\n- Add get_exit_codes() method\n\n## Validation Hook\n- Add check_server_health() in pytest_sessionfinish\n- Run after all deployments shut down\n- Check all exit codes from _exit_codes\n\n## Result Reporting\nJSON: Add \"server_health\" section with issues list\nJUnit: Options to discuss - synthetic test? testsuite error? system-err?\n\n## Exit Status\n- Set session.exitstatus = 1 if any exit code != 0\n- Consider special handling for sanitizer exit codes (134, etc.)","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-05T15:01:41.421520275+01:00","updated_at":"2025-11-06T12:40:57.240616243+01:00","closed_at":"2025-11-06T12:40:57.240616243+01:00","source_repo":"."}
{"id":"armadillo-41","content_hash":"b0fa56677a8d0cb345b2113bd794e9579f48350a1c32a2a469b3cbb35761214f","title":"Refactor deployment fixtures to package-scope only","description":"Simplify deployment fixture scoping to use ONLY package scope. This gives clear exit code ownership (package = test suite) and reasonable cost (one deployment per directory).\n\nCurrent state:\n- session-scoped: arango_single_server, arango_cluster, arango_deployment\n- function-scoped: arango_single_server_function, arango_cluster_function\n- Mixed usage across different scopes\n\nTarget state:\n- ONLY package-scoped fixtures: arango_single_server, arango_cluster, arango_deployment\n- Tests MUST be organized in packages (directories with conftest.py)\n- Root-level tests (not in a package) are forbidden\n- Clear 1:1 mapping: package directory = test suite = deployment = exit code ownership\n\nRationale:\n- KISS principle - one clear way to do things\n- Exit codes from server shutdown map cleanly to test suite\n- Reasonable cost (not per-file, not session-wide)\n- Can reintroduce other scopes later if truly needed","design":"## Changes Required\n\n### 1. Update Plugin Fixtures (armadillo/pytest_plugin/plugin.py)\n- Change all fixture scopes from \"session\"/\"function\" to \"package\"\n- Remove function-scoped variants (_function suffix)\n- Update deployment_id generation to include package path\n- Track deployments by package path instead of session-wide\n\n### 2. Update Deployment Tracking\n- Change _session_deployments to _package_deployments\n- Key by package path: Dict[Path, InstanceManager]\n- Cleanup deployments after each package completes\n\n### 3. Add Package Structure Validation\n- pytest_collection_modifyitems hook to validate structure\n- Error if test file not in a package (no conftest.py in parent)\n- Clear error message directing users to organize tests properly\n\n### 4. Update tests/conftest.py\n- Change scope=\"session\" to scope=\"package\" for adb, base_url fixtures\n- These will automatically scope to package level\n\n### 5. Update Documentation\n- README.md: Remove session/function fixture docs\n- Add package structure requirements\n- Show example directory structure with conftest.py\n\n### 6. Pytest Hooks for Exit Code Tracking\n- Track which package each deployment belongs to\n- Capture exit codes when package cleanup happens\n- Map exit codes to package/suite for result reporting","acceptance_criteria":"- All deployment fixtures use scope=\"package\"\n- No session or function scoped deployment fixtures remain\n- Tests in root level (no package) fail with clear error\n- Deployment exit codes map to package/test suite\n- All existing tests still pass with new scoping\n- Documentation updated to reflect package-only structure","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-05T15:21:08.415056942+01:00","updated_at":"2025-11-06T10:39:07.906665207+01:00","closed_at":"2025-11-06T10:39:07.906665207+01:00","source_repo":".","dependencies":[{"issue_id":"armadillo-41","depends_on_id":"armadillo-40","type":"blocks","created_at":"2025-11-05T15:21:08.425052294+01:00","created_by":"daemon"}]}
{"id":"armadillo-42","content_hash":"59eaed03a2bd00c8bc414a90e73f9b4da776c9ebf50bd80df9ada6f385a1223e","title":"Review and improve pytest marker system","description":"The current marker system automatically adds markers based on fixture usage and test names, but this could be improved and made more explicit.\n\nCurrent automatic marker logic:\n- Tests using arango_cluster → marked as \"slow\"\n- Tests with \"stress\" or \"load\" in name → marked as \"stress_test\" and \"slow\"\n- Tests with \"crash\" or \"fail\" in name → marked as \"crash_test\"\n- Tests with \"perf\", \"benchmark\", or \"timing\" in name → marked as \"performance\"\n\nIssues:\n- Implicit marker assignment based on naming conventions\n- No clear documentation of when markers are applied\n- Marker logic mixed with package structure validation\n- Unclear if all these heuristics are still needed/accurate\n\nConsiderations:\n- Should markers be explicit (@pytest.mark.slow) or automatic?\n- Are there better ways to organize tests than by markers?\n- How do markers interact with package-scoped deployments?\n- Should different packages have different default markers?","design":"## Options to Consider\n\n### Option 1: Keep Automatic Markers\n- Review and refine current heuristics\n- Document marker assignment rules clearly\n- Separate marker logic from validation logic\n\n### Option 2: Make Markers Explicit\n- Remove automatic marker assignment\n- Require explicit @pytest.mark.slow decorators\n- Clearer but more verbose\n\n### Option 3: Package-Level Markers\n- Use conftest.py to set markers for entire packages\n- e.g., all tests in stress_tests/ package are stress tests\n- Cleaner organization\n\n### Option 4: Hybrid Approach\n- Some markers automatic (e.g., cluster → slow)\n- Some markers explicit (e.g., @pytest.mark.flaky)\n- Balance convenience and clarity","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-05T15:50:28.818998693+01:00","updated_at":"2025-11-05T15:50:28.818998693+01:00","source_repo":"."}
{"id":"armadillo-43","content_hash":"9433daab9908572d74ba674af4ec89be8d1190cd59af94c7e9bd4b5643631a48","title":"Review and clean up hasattr usage patterns","description":"Review all hasattr() calls in the codebase to determine if they are legitimate or code smells.\n\nCode smell pattern: Using hasattr() to check for attributes that are always set in __init__\nLegitimate pattern: Checking for attributes that may not exist due to lifecycle/initialization order\n\nFound hasattr calls to review:\n- plugin.py: hasattr(_process_supervisor, \"_processes\") - likely code smell (set in __init__)\n- plugin.py: hasattr(request, \"node\") - likely legitimate (pytest API)\n- plugin.py: hasattr(report, \"crash_info\") - likely legitimate (dynamic attribute)\n- plugin.py: hasattr(signal, \"Signals\") - legitimate (Python version compatibility)\n- Other files: 16 more occurrences across utils, instances, testing modules\n\nGuidelines:\n1. If attribute is set in __init__, don't use hasattr - just access it\n2. If checking for None is needed, do explicit None check instead\n3. Add comments explaining why hasattr is necessary for legitimate cases\n4. Consider using try/except AttributeError for truly dynamic attributes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-05T16:42:41.835503346+01:00","updated_at":"2025-11-06T11:57:37.265788506+01:00","closed_at":"2025-11-06T11:57:37.265788506+01:00","source_repo":"."}
{"id":"armadillo-44","content_hash":"5049038c8a977e9c036732a3a7aabeb227e77a9f532327241b39c5328c937d41","title":"Add cluster-specific helper fixtures for accessing individual nodes","description":"During the package-scoped fixture refactoring, we removed the arango_coordinators, arango_dbservers, and arango_agents fixtures because they depended on the removed arango_cluster plugin fixture.\n\nHowever, these helper fixtures are useful for tests that need to access individual cluster nodes (e.g., testing failover, node-specific operations, etc.).\n\nWe should re-introduce these fixtures in a way that works with the new package-scoped pattern.","design":"Two possible approaches:\n\n**Option 1: Plugin-level helpers + package-level manager fixture**\n- Add _cluster_manager fixture to conftest_helpers.py that yields InstanceManager\n- Add arango_coordinators, arango_dbservers, arango_agents to plugin.py that depend on _cluster_manager\n- Plugin fixtures work because they depend on package-scoped _cluster_manager\n\n**Option 2: Extend conftest_helpers.py with cluster-specific factory**\n- Add create_cluster_fixtures() function to conftest_helpers.py\n- Returns (_cluster_manager, coordinators, dbservers, agents) fixtures\n- Packages that need cluster node access use this instead of create_package_fixtures()\n- Keeps all fixture creation logic in one place\n\nOption 2 is recommended for consistency and simplicity.","acceptance_criteria":"- Tests can access individual coordinators via fixture\n- Tests can access individual dbservers via fixture  \n- Tests can access individual agents via fixture\n- Fixtures work with package-scoped deployments\n- Pattern is documented in README.md","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-06T10:38:58.456258715+01:00","updated_at":"2025-11-06T10:38:58.456258715+01:00","source_repo":"."}
{"id":"armadillo-45","content_hash":"a522960f4be00f9d545a736a182b2354c74aac59773ee19549530d1038df5097","title":"Consider autouse fixture for package deployments","description":"Currently, package-scoped deployments are created lazily when a test requests the `adb` or `base_url` fixture. This means tests must explicitly declare these dependencies even if they don't directly use them.\n\n**Current approach:**\n```python\ndef test_something(adb):  # Must request adb to get deployment\n    # Even if test doesn't use adb directly\n    response = requests.get(\"http://...\")\n```\n\n**Alternative with autouse:**\n```python\ndef test_something():  # Deployment happens automatically\n    response = requests.get(\"http://...\")\n```\n\n**Pros of autouse:**\n- Simpler test signatures for tests that need deployment but don't use client\n- Clearer that deployment is package-wide, not per-test\n- No accidental missing deployment dependency\n\n**Cons of autouse:**\n- Less explicit about what resources tests use\n- May deploy even when not needed (though unlikely in practice)\n- Fixture dependency chain is less clear\n\n**Decision needed:** Should deployments be explicit (current) or implicit (autouse)?","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-06T11:38:21.182973878+01:00","updated_at":"2025-11-06T11:38:21.182973878+01:00","source_repo":".","labels":["design","fixtures","testing"]}
{"id":"armadillo-46","content_hash":"2f541441da916d65684255e1d682a6223fe0ad991327670bc1aeea5000ff50c9","title":"Implement diagnostic collection for timeout events","description":"Add diagnostic collection when test timeouts occur. The timeout_handler already has hooks (set_pre_terminate_hook) but they're not used yet. Should collect:\n- Server logs from temp_dir\n- Request coredumps from running processes\n- Capture process states (ps, lsof, etc.)\n- Save test artifacts\n\nLocation: armadillo/cli/commands/test.py line 340","status":"open","priority":3,"issue_type":"feature","created_at":"2025-11-10T10:43:54.106420181+01:00","updated_at":"2025-11-10T10:43:54.106420181+01:00","source_repo":"."}
{"id":"armadillo-47","content_hash":"4e446a69d8b1c17d9328a7339400f18fd42c251066399a964745dca170d711e1","title":"Simplify DeploymentStrategy protocol - remove redundant verify_readiness","description":"The DeploymentStrategy protocol forces a verify_readiness() method that doesn't fit cluster deployments naturally. ClusterStrategy's implementation is a no-op because the ClusterBootstrapper already verifies readiness internally during bootstrap_cluster().\n\nOptions:\n1. Remove verify_readiness() from Protocol - let orchestrator handle verification differently per mode\n2. Make ClusterStrategy delegate verification to bootstrapper explicitly\n3. Refactor orchestrator to not call verify_readiness() separately when already verified\n\nCurrent unused parameters in ClusterStrategy.verify_readiness:\n- servers: Dict[str, ArangoServer]  \n- timeout: float\n\nLocation: armadillo/instances/deployment_strategy.py and deployment_orchestrator.py","design":"Consider removing verify_readiness from the Protocol interface entirely since cluster mode already verifies during bootstrap and single-server mode could verify inline during start_servers(). This would make the interface more honest about what each strategy actually does.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-10T11:34:36.292727137+01:00","updated_at":"2025-11-10T15:11:56.187953539+01:00","closed_at":"2025-11-10T15:11:56.187953539+01:00","source_repo":"."}
{"id":"armadillo-48","content_hash":"8732d0cd1c8e4f26615cf871483c2990160dff7fd1547522154acf7fa824ecab","title":"Evaluate pytest-scoped singletons (_plugin, _reporter)","description":"Currently have module-level singletons in pytest plugin code:\n- _plugin in pytest_plugin/plugin.py\n- _reporter in pytest_plugin/reporter.py\n\nThese are initialized once per test session and required by pytest's plugin architecture.\n\nNeed to evaluate:\n1. Are these truly necessary given pytest's plugin system?\n2. Could they be managed through pytest's own mechanisms (config, session)?\n3. Is there a cleaner way to handle test session state?\n4. Document why they exist if they must remain","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-10T13:54:12.320424409+01:00","updated_at":"2025-11-10T14:43:28.902071763+01:00","closed_at":"2025-11-10T14:43:28.902071763+01:00","source_repo":".","labels":["architecture","globals","pytest"]}
{"id":"armadillo-49","content_hash":"e6d71a68f012a818d184035509fbb0c0f1f6c08ad5a18fe42822408c7c679572","title":"Refactor deployment architecture: eliminate pass-through layers, strategies own lifecycle","description":"Currently DeploymentStrategy.start_servers() receives both a plan and a dict of server instances, which is confusing:\n\n- Plan contains ServerConfig objects (specifications)\n- servers dict contains ArangoServer instances (runtime objects)\n- Both are passed to start_servers() but only servers dict is actually used\n- Plan is only used for isinstance() type checking\n\nThe plan should be the source of truth, but currently the strategy needs runtime server instances that aren't part of the plan.\n\nNeed to clarify:\n1. Should plan hold both config AND created instances?\n2. Should strategy get instances from registry instead?\n3. Should strategy create instances itself from plan + factory?\n\nLocation: deployment_strategy.py and deployment_orchestrator.py","design":"## Current Architecture Problems\n\n### Dependency Pass-Through Chain\n- InstanceManager creates ClusterBootstrapper, only to pass it to DeploymentOrchestrator\n- DeploymentOrchestrator receives it, only to pass it to ClusterStrategy\n- Only ClusterStrategy actually uses it\n- Same pattern with HealthMonitor and ServerRegistry\n\n### Strategies Are Thin Wrappers (Not Strategic)\nCurrent strategies don't provide different algorithms - they just:\n1. Validate plan type (isinstance check - redundant, orchestrator already knows!)\n2. Delegate to actual workers:\n   - SingleServerStrategy → calls server.start() directly\n   - ClusterStrategy → delegates to ClusterBootstrapper.bootstrap_cluster()\n\n**The real work happens elsewhere:**\n- ArangoServer.start() - actual single server startup\n- ClusterBootstrapper.bootstrap_cluster() - actual cluster startup\n\n### Redundant Information Flow\n- Plan contains ServerConfig objects (specifications)\n- Orchestrator creates ArangoServer instances from plan using factory\n- Orchestrator stores instances in registry, then immediately retrieves them\n- Passes both plan AND servers dict to strategy\n- Strategy only uses servers dict (plan only for isinstance check)\n\n### ServerRegistry Is Barely Used\nJust a dict wrapper providing minimal value:\n- Used to store servers after creation\n- Immediately retrieved to pass to strategy\n- Used again during shutdown for ordering\n- Could be replaced with plain dict\n\n## Proposed Clean Architecture\n\n### Core Principle: Strategies Own Complete Lifecycle\n\n```python\n# No protocol needed - different dependencies per type\nclass SingleServerDeployment:\n    def __init__(self, factory: ServerFactory, logger: Logger):\n        self._factory = factory\n        self._logger = logger\n    \n    def deploy(self, plan: SingleServerDeploymentPlan, timeout: float):\n        # Create server from plan\n        servers = self._factory.create_server_instances([plan.server])\n        server = next(iter(servers.values()))\n        \n        # Start and verify\n        server.start(timeout)\n        health = server.health_check_sync(timeout)\n        if not health.is_healthy:\n            raise ServerError(...)\n        \n        return {server.server_id: server}\n\nclass ClusterDeployment:\n    def __init__(\n        self, \n        factory: ServerFactory, \n        bootstrapper: ClusterBootstrapper,\n        logger: Logger\n    ):\n        self._factory = factory\n        self._bootstrapper = bootstrapper\n        self._logger = logger\n    \n    def deploy(self, plan: ClusterDeploymentPlan, timeout: float):\n        # Create all servers\n        servers = self._factory.create_server_instances(plan.servers)\n        \n        # Bootstrap (starts + verifies)\n        startup_order = []\n        self._bootstrapper.bootstrap_cluster(servers, startup_order, timeout)\n        \n        return servers\n```\n\n### Simplified Orchestrator\n\n```python\nclass DeploymentOrchestrator:\n    def __init__(\n        self,\n        logger: Logger,\n        server_factory: ServerFactory,\n        cluster_bootstrapper: Optional[ClusterBootstrapper] = None,\n        health_monitor: Optional[HealthMonitor] = None,\n    ):\n        self._logger = logger\n        self._factory = server_factory\n        self._bootstrapper = cluster_bootstrapper\n        self._health_monitor = health_monitor\n        self._servers: Dict[ServerId, ArangoServer] = {}  # Plain dict\n        self._startup_order: List[ServerId] = []\n    \n    def execute_deployment(self, plan: DeploymentPlan, timeout: float):\n        # Create strategy with dependencies it needs\n        if isinstance(plan, SingleServerDeploymentPlan):\n            strategy = SingleServerDeployment(self._factory, self._logger)\n        else:\n            strategy = ClusterDeployment(\n                self._factory, \n                self._bootstrapper, \n                self._logger\n            )\n        \n        # Strategy owns full deployment\n        self._servers = strategy.deploy(plan, timeout)\n        \n        # Optional: health monitor for extra validation\n        if self._health_monitor:\n            health = self._health_monitor.check_deployment_health(self._servers)\n            if not health.is_healthy:\n                raise ServerError(...)\n    \n    def shutdown_deployment(self, timeout: float):\n        # Orchestrator handles shutdown\n        for server in self._shutdown_order(self._servers.values()):\n            server.stop(timeout)\n        self._servers.clear()\n```\n\n### InstanceManager Simplifies\n\nInstanceManager should only create components it directly uses:\n- Create ClusterBootstrapper (for cluster deployments)\n- Pass to orchestrator (which passes to cluster strategy)\n\nDon't create components just to pass through layers.\n\n## Key Benefits\n\n1. **Clear Ownership**: Strategy creates, starts, verifies - full lifecycle\n2. **No Pass-Through**: Each component only receives what it uses\n3. **Explicit Dependencies**: Strategy gets factory injected (testable!)\n4. **No Registry**: Just plain dict for shutdown ordering\n5. **No Protocol**: Different strategies have different needs\n6. **Replaceable**: Each component is clean black box\n\n## Black Box Boundaries\n\n- **Plan**: Immutable specification (ServerConfig objects)\n- **Strategy**: Deployment algorithm (owns create → start → verify)\n- **Factory**: Creates ArangoServer instances from ServerConfig\n- **Bootstrapper**: Cluster-specific sequencing logic\n- **Orchestrator**: Coordinates deployment + handles shutdown\n- **InstanceManager**: Public API facade\n\n## Open Questions\n\n1. Should InstanceManager create ClusterBootstrapper at all, or should orchestrator create it on-demand?\n2. Should strategies return both servers dict AND startup_order?\n3. Should health_monitor validation happen inside strategies or in orchestrator?","notes":"This architectural refactoring came from analyzing issue #armadillo-47 (verify_readiness removal) and discovering the strategy pattern was being misused.\n\nCurrent code location: \n- armadillo/instances/deployment_strategy.py (thin wrappers)\n- armadillo/instances/deployment_orchestrator.py (creates strategy, creates servers, passes both)\n- armadillo/instances/manager.py (creates components just to pass through)\n\nRelated: Need to eliminate ServerRegistry (just use dict) as part of this refactor.\n\n**Starting implementation**: First step - eliminate ClusterBootstrapper pass-through. ClusterStrategy will create its own bootstrapper instead of receiving it through InstanceManager → Orchestrator → Strategy chain.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-10T15:10:40.927506822+01:00","updated_at":"2025-11-25T16:06:05.10679618+01:00","closed_at":"2025-11-25T16:06:05.106798795+01:00","source_repo":".","labels":["architecture","deployment"]}
{"id":"armadillo-692","content_hash":"2dbef70c36bd557614dcc28b52c553837293e50197e542763ef76f275c684e5b","title":"Add property-based tests for value objects","description":"Value objects (ServerId, DeploymentId, ServerContext) would benefit from property-based testing using hypothesis. Test edge cases, validation boundaries, and invariants systematically.","acceptance_criteria":"Property-based tests for: ServerId validation; DeploymentId validation; hash/equality properties; immutability properties","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-14T12:36:54.351241343+01:00","updated_at":"2025-11-14T12:36:54.351241343+01:00","source_repo":"."}
{"id":"armadillo-6ny","content_hash":"297e7a547cc45053c4b6f59a636c9a9f2c446bb0e0ffed3f8208425f61f99e10","title":"Add edge case tests to DeploymentFactory","description":"DeploymentFactory tests are missing coverage for important edge cases:\n\nMissing tests:\n1. None server_args handling\n2. Empty dict server_args ({})\n3. Minimum cluster config (1-1-1)\n4. Large cluster configurations (e.g., 5 agents, 10 dbservers, 3 coordinators)\n5. Conflicting arguments (e.g., custom port that should be ignored)\n\nThese edge cases ensure the factory handles all valid inputs correctly and fails gracefully on invalid ones.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-25T14:12:34.375102895+01:00","updated_at":"2025-11-25T16:47:09.862970081+01:00","closed_at":"2025-11-25T16:47:09.862974339+01:00","source_repo":".","labels":["coverage","testing"]}
{"id":"armadillo-8qv","content_hash":"fef83c97f2a14acc2bd7d0abde038b20821b7006009685c05cd3e18f86190e69","title":"Add resource cleanup on failure tests","description":"Test that resources (ports, processes, temp directories) are properly cleaned up when deployment fails. Check for port leaks, orphaned processes, and filesystem cleanup.","acceptance_criteria":"Tests verify: ports released on failure; processes killed on failure; temp dirs cleaned up; no resource leaks after failed deployments","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-14T12:35:41.525383997+01:00","updated_at":"2025-11-14T14:07:07.204719881+01:00","closed_at":"2025-11-14T14:07:07.204719881+01:00","source_repo":"."}
{"id":"armadillo-9","content_hash":"7b64dcfa5d870ddc439a027d39c0dbfe7aa290c01acf7c67017bd0592a28317a","title":"Establish consistent async/sync patterns","description":"Codebase has mixed sync/async patterns causing confusion and potential performance issues. Need to choose one paradigm and apply consistently, with explicit adapters where needed.","design":"Decision: Go full async where appropriate (I/O-bound operations)\n\n1. Audit sync/async boundary:\n   - Mark each module as Pure Async, Pure Sync, or Mixed\n   - Identify which modules need conversion\n   \n2. Convert core classes to async:\n   - instances/server.py (start, stop, health checks)\n   - instances/manager.py (deployment operations)\n   - instances/health_checker.py (all checks)\n   \n3. Create sync adapters module:\n   - sync_adapters.py with explicit sync wrappers\n   - Document when/why to use each adapter\n   - Warning about event loop creation costs\n   \n4. Update tests:\n   - Use @pytest.mark.asyncio for async tests\n   - Update fixtures to support async\n   - Document async testing patterns\n   \n5. Keep pure computation sync (utils, validation, etc.)","acceptance_criteria":"- Clear async/sync boundary documented\n- Core I/O operations are async\n- Sync adapters isolated in one module\n- Tests updated to use async patterns\n- No asyncio.run() in class methods\n- All tests passing","notes":"DEFERRED - Complex pytest-asyncio integration issues\n\nInvestigation findings (Oct 27, 2025):\n\n**Test Integration Problem:**\nWhen converting tests to @pytest.mark.asyncio with async/await:\n- Individual async tests PASS when run alone\n- Multiple async tests together FAIL during pytest-asyncio setup\n- Error: PermissionError: [Errno 1] Operation not permitted (in selectors.py during event loop creation)\n- Also: AttributeError: '_UnixSelectorEventLoop' object has no attribute '_signal_handlers' (during cleanup)\n\n**Environment:**\n- Python 3.13.3\n- pytest-asyncio 1.1.0\n- WSL2\n- ulimit -n: 1048576 (NOT resource exhaustion)\n\n**What Was Tried:**\n1. asyncio.run() in sync tests - caused event loop conflicts\n2. Creating server instances in async test context - event loop cleanup errors\n3. Module-scoped event loops - not fully tested\n4. Reverting to sync tests - still got setup errors\n\n**Root Cause:**\nUNKNOWN. The interaction between pytest-asyncio, Python 3.13, test class fixtures (setup_method), and event loop management is complex.\n\n**Decision:**\nAsync conversion work has been rolled back (git reset to 634470b1ebb).\nCurrent synchronous approach works perfectly: 494 tests passing, 2 skipped.\n\n**To Revisit:**\n- Investigate pytest-asyncio event loop management in depth\n- Consider alternative testing approaches\n- Check if Python 3.14 or newer pytest-asyncio versions resolve the issue\n- May need to restructure test classes to avoid setup_method with async tests","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-25T11:24:56.748859382+02:00","updated_at":"2025-10-27T14:19:35.244487531+01:00","source_repo":"."}
{"id":"armadillo-aom","content_hash":"48e24d011b1ab5d6b0f7281d6f5294ffc5ad0435bde42792a0ee102fd02b87c5","title":"Remove global ProcessSupervisor singleton in favor of ApplicationContext","description":"Current architecture has ProcessSupervisor as both a global singleton and ApplicationContext dependency. This hybrid state is confusing and error-prone.\n\nNeed to:\n1. Remove global helper functions (start_supervised_process, stop_supervised_process, etc)\n2. Update server.py to use app_context.process_supervisor directly\n3. Update health_checker.py to use app_context.process_supervisor directly\n4. Remove global _process_supervisor singleton\n5. Update all tests\n\nThis completes the dependency injection refactoring started with ApplicationContext.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T15:01:09.626752132+01:00","updated_at":"2025-11-28T10:31:43.929135435+01:00","closed_at":"2025-11-28T10:31:43.929135435+01:00","source_repo":"."}
{"id":"armadillo-bt7","content_hash":"b493d1315c705fed4d06fb5c724c3dd233ba173214bc654fc26d4f3522ec3e1f","title":"Fix over-mocking in DeploymentFactory tests","description":"DeploymentFactory tests (lines 28-178) mock ArangoServer.create_* methods and verify call arguments, testing implementation details instead of behavior. This makes tests brittle and coupled to implementation.\n\nRefactor tests to:\n- Test actual deployment behavior (server creation, port allocation, configuration)\n- Remove mocks of ArangoServer.create_* methods where possible\n- Verify actual state instead of mock call arguments\n\nExample: Instead of mocking create_single_server and checking config.port == 0, create actual deployment and verify deployment.server.port \u003e 0","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-25T14:12:26.455036043+01:00","updated_at":"2025-11-25T16:47:09.869178222+01:00","closed_at":"2025-11-25T16:47:09.869180726+01:00","source_repo":".","labels":["quality","testing"]}
{"id":"armadillo-e2o","content_hash":"d96010a6ea825fb26833b218a54d41ea7f1f0fcbed6ae6fb3ba5ea99e223f46b","title":"Consolidate deployment plan tests","description":"Deployment plan tests have some overlap and could be consolidated. TestDeploymentPlan class mixes unit and integration concerns. Separate pure data structure tests from behavior tests.","acceptance_criteria":"Deployment plan tests reorganized; clear separation between structure and behavior; no duplicate coverage","status":"open","priority":4,"issue_type":"chore","created_at":"2025-11-14T12:36:54.704131954+01:00","updated_at":"2025-11-14T12:36:54.704131954+01:00","source_repo":"."}
{"id":"armadillo-elo","content_hash":"c9ddb608432e4ad9356a01ac9e850f46c902b0723e3633a70132aef0cdf520bd","title":"Strengthen assertions in DeploymentFactory tests","description":"Some tests in DeploymentFactory have weak assertions that could be more explicit and provide better error messages.\n\nExamples:\n1. Uniqueness checks (line 193-194): Instead of 'assert len(server_ids) == len(set(server_ids))', verify exact expected IDs\n2. Server role verification: When checking agents, also verify they have ServerRole.AGENT\n3. Server ID format: Verify server IDs follow expected pattern (deployment-id-role-N)\n\nBetter assertions:\n- Provide clearer failure messages\n- Verify more specific behaviors\n- Make test intent more obvious\n\nThis improves test maintainability and debugging when tests fail.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-25T14:12:43.608722749+01:00","updated_at":"2025-11-25T16:47:09.871628704+01:00","closed_at":"2025-11-25T16:47:09.871630297+01:00","source_repo":".","labels":["quality","testing"]}
{"id":"armadillo-mjr","content_hash":"30fc7758c362b3a93575b8e37cab50b36c2bed50dfa5bf51e81235f604521700","title":"Add deployment state transition tests","description":"Test deployment lifecycle state transitions: none → deploying → deployed → shutting_down → none. Verify state consistency during transitions and that redeployment after shutdown works correctly.","acceptance_criteria":"Tests verify: deploy → shutdown → deploy works; state is consistent; errors during transitions are handled; deployment state persists correctly","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-14T12:35:36.152630064+01:00","updated_at":"2025-11-14T14:56:03.466078018+01:00","closed_at":"2025-11-14T14:56:03.466078018+01:00","source_repo":"."}
